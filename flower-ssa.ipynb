{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1941538,"sourceType":"datasetVersion","datasetId":1158171},{"sourceId":2016746,"sourceType":"datasetVersion","datasetId":1207074},{"sourceId":2488051,"sourceType":"datasetVersion","datasetId":1506056},{"sourceId":2488062,"sourceType":"datasetVersion","datasetId":1506064},{"sourceId":3303357,"sourceType":"datasetVersion","datasetId":1998107}],"dockerImageVersionId":30120,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"5\"\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\ntqdm.pandas()\nimport Levenshtein\nimport cv2\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport time\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nfrom torch.nn.utils.rnn import pack_padded_sequence\nimport torchvision\nimport PIL.Image as Image\nfrom torchvision.transforms import ToTensor, ToPILImage\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nimport torch\nfrom torch import nn\nfrom nltk.translate.bleu_score import corpus_bleu\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-05-13T16:13:34.31829Z","iopub.execute_input":"2022-05-13T16:13:34.318747Z","iopub.status.idle":"2022-05-13T16:13:37.73948Z","shell.execute_reply.started":"2022-05-13T16:13:34.318629Z","shell.execute_reply":"2022-05-13T16:13:37.738311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Put the image paths and caption paths into lists to facilitate their usage during the loading and processing of the dataset, especially for handling both train and test sets.</h3>","metadata":{}},{"cell_type":"code","source":"class_id = []\nwith os.scandir('../input/oxford-102flowers/data/train') as entries:\n    for entry in entries:\n        class_id.append(int(entry.name))\n\nlist_caption = []\nlist_image = []\ntrain_list_image = []\ntrain_class_id =[]\ntest_list_image = []\ntrain_caption = []\ntext_path = \"../input/text-description/text_arab/\"\ntrian_path = \"../input/oxford-102flowers/data/train\"\nwith os.scandir('../input/oxford-102flowers/data/train') as entries:\n    for entry in entries:\n        d=os.path.join(trian_path,entry.name)\n        with os.scandir(d) as img:\n            for m in img:\n                text = m.name[:-4]\n                text = text +\".txt\"\n                image_train_path=os.path.join(d,m.name)\n                train_list_image.append(image_train_path)\n                train_class_id.append(int(entry.name))\n                train_caption.append(text_path+text)\nvalid_path = \"../input/oxford-102flowers/data/valid\"\nwith os.scandir('../input/oxford-102flowers/data/valid') as entries:\n    for entry in entries:\n        d=os.path.join(valid_path,entry.name)\n        with os.scandir(d) as img:\n            for m in img:\n                text = m.name[:-4]\n                text = text +\".txt\"\n                image_train_path=os.path.join(d,m.name)\n                train_list_image.append(image_train_path)\n                train_class_id.append(int(entry.name))\n                train_caption.append(text_path+text)\n                \ntest_caption = []\ntest_list_image = []\ntest_class_id =[]\ntest_path = \"../input/oxford-102flowers/data/test\"\nwith os.scandir('../input/oxford-102flowers/data/test') as entries:\n    for entry in entries:\n        d=os.path.join(test_path,entry.name)\n        with os.scandir(d) as img:\n            for m in img:\n                text = m.name[:-4]\n                text = text +\".txt\"\n                image_train_path=os.path.join(d,m.name)\n                test_list_image.append(image_train_path)\n                test_class_id.append(int(entry.name))\n                test_caption.append(text_path + text)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T23:32:41.640753Z","iopub.execute_input":"2022-03-15T23:32:41.641107Z","iopub.status.idle":"2022-03-15T23:32:42.899169Z","shell.execute_reply.started":"2022-03-15T23:32:41.641079Z","shell.execute_reply":"2022-03-15T23:32:42.898405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(train_list_image[:5])\nprint(train_class_id[:5])\nprint(train_caption[:5])\nprint(test_list_image[:5])\nprint(test_class_id[:5])\nprint(test_caption[:5])","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:06:15.616769Z","iopub.execute_input":"2022-03-14T22:06:15.617118Z","iopub.status.idle":"2022-03-14T22:06:15.623639Z","shell.execute_reply.started":"2022-03-14T22:06:15.617088Z","shell.execute_reply":"2022-03-14T22:06:15.62272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_list_image))\nprint(len(train_class_id))\nprint(len(train_caption))\nprint(len(test_list_image))\nprint(len(test_class_id))\nprint(len(test_caption))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:06:17.018565Z","iopub.execute_input":"2022-03-14T22:06:17.018904Z","iopub.status.idle":"2022-03-14T22:06:17.027045Z","shell.execute_reply.started":"2022-03-14T22:06:17.018871Z","shell.execute_reply":"2022-03-14T22:06:17.025899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyarabic\nimport pyarabic.araby as araby\nimport pyarabic.number as number","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:06:18.149961Z","iopub.execute_input":"2022-03-14T22:06:18.150282Z","iopub.status.idle":"2022-03-14T22:06:25.301102Z","shell.execute_reply.started":"2022-03-14T22:06:18.150253Z","shell.execute_reply":"2022-03-14T22:06:25.300196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Clean and toknize the Arabic text</h3>","metadata":{}},{"cell_type":"code","source":"tokens_new = []\ncaptions_paths = train_caption + test_caption\nfor i in captions_paths:\n    with open(i, \"r\") as f:\n        captions = f.read().split('\\n')\n    for cap in captions:\n        cap = araby.strip_tashkeel(cap)\n        cap = araby.strip_lastharaka(cap)\n        cap = araby.normalize_hamza(cap)\n        cap = araby.tokenize(cap)\n        for s in cap:\n            if not s.startswith('و') and not s.startswith('.') and not s.startswith('�') and not s.startswith(','):\n                if s not in tokens_new:\n                    tokens_new.append(s)\nprint(len(tokens_new))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:06:26.568263Z","iopub.execute_input":"2022-03-14T22:06:26.568591Z","iopub.status.idle":"2022-03-14T22:07:33.713471Z","shell.execute_reply.started":"2022-03-14T22:06:26.568557Z","shell.execute_reply":"2022-03-14T22:07:33.712547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def line_to_sequence(caption_path):\n    bag_word = []\n    with open(caption_path, \"r\") as f:\n        captions = f.read().split('\\n')\n    for cap in captions:\n        cap = araby.strip_tashkeel(cap)\n        cap = araby.strip_lastharaka(cap)\n        cap = araby.normalize_hamza(cap)\n        cap = araby.tokenize(cap)\n        for s in cap:\n            if not s.startswith('و') and not s.startswith('.') and not s.startswith('�') and not s.startswith(','):\n                bag_word.append(s)\n    return bag_word\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:07:41.847304Z","iopub.execute_input":"2022-03-14T22:07:41.847635Z","iopub.status.idle":"2022-03-14T22:07:41.854169Z","shell.execute_reply.started":"2022-03-14T22:07:41.847604Z","shell.execute_reply":"2022-03-14T22:07:41.853248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(tokens_new))\nprint(tokens_new[:100])\n","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:07:43.445189Z","iopub.execute_input":"2022-03-14T22:07:43.445523Z","iopub.status.idle":"2022-03-14T22:07:43.450888Z","shell.execute_reply.started":"2022-03-14T22:07:43.445492Z","shell.execute_reply":"2022-03-14T22:07:43.449687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Add the End and Strat for each caption.</h3>","metadata":{}},{"cell_type":"code","source":"        ixtoword = {}\n        ixtoword[0] = '<end>'\n        ixtoword[1] = '<out>'\n        wordtoix = {}\n        wordtoix['<end>'] = 0\n        wordtoix['<out>'] = 1\n        ix = 2\n        for w in tokens_new:\n            wordtoix[w] = ix\n            ixtoword[ix] = w\n            ix += 1","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:07:44.296545Z","iopub.execute_input":"2022-03-14T22:07:44.29688Z","iopub.status.idle":"2022-03-14T22:07:44.305397Z","shell.execute_reply.started":"2022-03-14T22:07:44.296847Z","shell.execute_reply":"2022-03-14T22:07:44.30434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass tokinazer():\n    def __init__(self,wordtoix,ixtoword):\n        super(tokinazer, self).__init__()\n        self.wordtoix = wordtoix\n        self.ixtoword = ixtoword\n        \n    def encode(self,seq):\n        return [self.wordtoix[i] for i in seq]\n    def decode(self,seq):\n        return [self.ixtoword[i] for i in seq]\ntokinazer = tokinazer(wordtoix,ixtoword)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:07:45.910264Z","iopub.execute_input":"2022-03-14T22:07:45.910576Z","iopub.status.idle":"2022-03-14T22:07:45.916195Z","shell.execute_reply.started":"2022-03-14T22:07:45.910549Z","shell.execute_reply":"2022-03-14T22:07:45.915034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Determine the maximum text length and use it as the sequences length.</h3>","metadata":{"execution":{"iopub.status.busy":"2024-03-03T19:39:34.219615Z","iopub.execute_input":"2024-03-03T19:39:34.220245Z","iopub.status.idle":"2024-03-03T19:39:34.233076Z","shell.execute_reply.started":"2024-03-03T19:39:34.220137Z","shell.execute_reply":"2024-03-03T19:39:34.231667Z"}}},{"cell_type":"code","source":"s = train_caption + test_caption\nmax_seq = 0\nfor i in s:\n    seq = line_to_sequence(i)\n    seq = tokinazer.encode(seq)\n    if max_seq < len(seq):\n        max_seq = len(seq)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:07:47.040426Z","iopub.execute_input":"2022-03-14T22:07:47.040839Z","iopub.status.idle":"2022-03-14T22:07:57.390965Z","shell.execute_reply.started":"2022-03-14T22:07:47.040779Z","shell.execute_reply":"2022-03-14T22:07:57.390078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:08:08.225109Z","iopub.execute_input":"2022-03-14T22:08:08.225524Z","iopub.status.idle":"2022-03-14T22:08:08.235327Z","shell.execute_reply.started":"2022-03-14T22:08:08.225484Z","shell.execute_reply":"2022-03-14T22:08:08.23402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Pre-process of the iamges and thier coptions</h3>","metadata":{}},{"cell_type":"code","source":"def text_to_tensor(path,max_len):\n    seq = line_to_sequence(path)\n    seq = tokinazer.encode(seq)\n    seq.append(0)\n    len_seq = len(seq)\n    tensor = torch.LongTensor(F.pad(torch.LongTensor(seq), pad=(0, (max_len+1 )- len_seq) , mode='constant', value=0)) \n    return tensor,len_seq\n\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, 'rb') as f:\n        img = Image.open(f)\n        return img.convert('RGB')\n    \n        \ntrain_transforms =  transforms.Compose([\n        transforms.Resize(256),\n        transforms.RandomCrop(256),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        #transforms.Normalize([0.5], [0.5]),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n\nclass dataset(torch.utils.data.Dataset):\n\n    def __init__(self,images_paths,captions_paths,max_len,class_id,transform=None):\n        self.images_paths = images_paths\n        self.captions_paths = captions_paths\n        self.max_len = max_len \n        self.class_id = class_id\n        self.transform = transform \n         \n    #dataset length\n    def __len__(self):\n        self.filelength = len(self.images_paths)\n        return self.filelength\n    \n    #load an one of images\n    def __getitem__(self,idx):\n     #   img_path = self.file_list[idx]\n        img = pil_loader(self.images_paths[idx].strip())\n        tensor_caption,len_seq = text_to_tensor(self.captions_paths[idx],self.max_len)\n        #seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long(\n        img_transformed = self.transform(img)\n        id_ = self.class_id[idx]\n        #img_transformed = img_transformed.expand(3,256,256)\n        return img_transformed,tensor_caption,len_seq,id_","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:08:09.273259Z","iopub.execute_input":"2022-03-14T22:08:09.273594Z","iopub.status.idle":"2022-03-14T22:08:09.289219Z","shell.execute_reply.started":"2022-03-14T22:08:09.273565Z","shell.execute_reply":"2022-03-14T22:08:09.287567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_list_image))\nprint(len(train_class_id))\nprint(len(train_caption))\nprint(len(test_list_image))\nprint(len(test_class_id))\nprint(len(test_caption))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:08:10.552915Z","iopub.execute_input":"2022-03-14T22:08:10.553239Z","iopub.status.idle":"2022-03-14T22:08:10.559933Z","shell.execute_reply.started":"2022-03-14T22:08:10.553211Z","shell.execute_reply":"2022-03-14T22:08:10.558332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_paths  = train_list_image[:7168]\ncaptions_paths = train_caption[:7168]\nclass_id_train = train_class_id[:7168] \ntest_images = test_list_image[:992] \ntest_captions = test_caption[:992]\nclass_id_test = test_class_id[:992]\ntrain_data = dataset(images_paths,captions_paths,max_seq,class_id_train,transform=train_transforms)\ntrain_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size=32, shuffle=True)\ntest_data = dataset(test_images,test_captions,max_seq,class_id_test,transform=train_transforms)\ntest_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size=32, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:08:11.359608Z","iopub.execute_input":"2022-03-14T22:08:11.359952Z","iopub.status.idle":"2022-03-14T22:08:11.366393Z","shell.execute_reply.started":"2022-03-14T22:08:11.359921Z","shell.execute_reply":"2022-03-14T22:08:11.365553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_data))\nprint(len(train_loader))\nprint(len(test_data))\nprint(len(test_loader))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:08:12.614524Z","iopub.execute_input":"2022-03-14T22:08:12.614861Z","iopub.status.idle":"2022-03-14T22:08:12.621169Z","shell.execute_reply.started":"2022-03-14T22:08:12.61481Z","shell.execute_reply":"2022-03-14T22:08:12.620136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:08:13.520661Z","iopub.execute_input":"2022-03-14T22:08:13.521026Z","iopub.status.idle":"2022-03-14T22:08:13.526017Z","shell.execute_reply.started":"2022-03-14T22:08:13.520995Z","shell.execute_reply":"2022-03-14T22:08:13.525097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Define the text Encoder to produce the word and sentence Embedding</h3>","metadata":{"execution":{"iopub.status.busy":"2024-03-03T19:41:45.513203Z","iopub.execute_input":"2024-03-03T19:41:45.513540Z","iopub.status.idle":"2024-03-03T19:41:45.518824Z","shell.execute_reply.started":"2024-03-03T19:41:45.513513Z","shell.execute_reply":"2024-03-03T19:41:45.517684Z"}}},{"cell_type":"code","source":"from torch.autograd import Variable\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nclass RNN_ENCODER(nn.Module):\n    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n                 nhidden=128, nlayers=1, bidirectional=True):\n        super(RNN_ENCODER, self).__init__()\n        self.n_steps = 18\n        self.ntoken = ntoken  # size of the dictionary\n        self.ninput = ninput  # size of each embedding vector\n        self.drop_prob = drop_prob  # probability of an element to be zeroed\n        self.nlayers = nlayers  # Number of recurrent layers\n        self.bidirectional = bidirectional\n        self.rnn_type = 'LSTM'\n        if bidirectional:\n            self.num_directions = 2\n        else:\n            self.num_directions = 1\n        # number of features in the hidden state\n        self.nhidden = nhidden // self.num_directions\n\n        self.define_module()\n        self.init_weights()\n\n    def define_module(self):\n        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n        self.drop = nn.Dropout(self.drop_prob)\n        if self.rnn_type == 'LSTM':\n            # dropout: If non-zero, introduces a dropout layer on\n            # the outputs of each RNN layer except the last layer\n            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n                               self.nlayers, batch_first=True,\n                               dropout=self.drop_prob,\n                               bidirectional=self.bidirectional)\n        elif self.rnn_type == 'GRU':\n            self.rnn = nn.GRU(self.ninput, self.nhidden,\n                              self.nlayers, batch_first=True,\n                              dropout=self.drop_prob,\n                              bidirectional=self.bidirectional)\n        else:\n            raise NotImplementedError\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        # Do not need to initialize RNN parameters, which have been initialized\n        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n        # self.decoder.weight.data.uniform_(-initrange, initrange)\n        # self.decoder.bias.data.fill_(0)\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters()).data\n        if self.rnn_type == 'LSTM':\n            return (Variable(weight.new(self.nlayers * self.num_directions,\n                                        bsz, self.nhidden).zero_()),\n                    Variable(weight.new(self.nlayers * self.num_directions,\n                                        bsz, self.nhidden).zero_()))\n        else:\n            return Variable(weight.new(self.nlayers * self.num_directions,\n                                       bsz, self.nhidden).zero_())\n\n    def forward(self, captions, cap_lens, hidden, mask=None):\n        # input: torch.LongTensor of size batch x n_steps\n        # --> emb: batch x n_steps x ninput\n        emb = self.drop(self.encoder(captions))\n        #\n        # Returns: a PackedSequence object\n        cap_lens = cap_lens.data.tolist()\n        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n        # tensor containing the initial hidden state for each element in batch.\n        # #output (batch, seq_len, hidden_size * num_directions)\n        # #or a PackedSequence object:\n        # tensor containing output features (h_t) from the last layer of RNN\n        output, hidden = self.rnn(emb, hidden)\n        # PackedSequence object\n        # --> (batch, seq_len, hidden_size * num_directions)\n        output = pad_packed_sequence(output, batch_first=True)[0]\n        # output = self.drop(output)\n        # --> batch x hidden_size*num_directions x seq_len\n        words_emb = output.transpose(1, 2)\n        # --> batch x num_directions*hidden_size\n        if self.rnn_type == 'LSTM':\n            sent_emb = hidden[0].transpose(0, 1).contiguous()\n        else:\n            sent_emb = hidden.transpose(0, 1).contiguous()\n        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n        return words_emb, sent_emb","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:08:14.525806Z","iopub.execute_input":"2022-03-14T22:08:14.526175Z","iopub.status.idle":"2022-03-14T22:08:14.542998Z","shell.execute_reply.started":"2022-03-14T22:08:14.526144Z","shell.execute_reply":"2022-03-14T22:08:14.542029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_encoder = RNN_ENCODER(len(tokens_new), nhidden=256)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:08:15.878712Z","iopub.execute_input":"2022-03-14T22:08:15.87922Z","iopub.status.idle":"2022-03-14T22:08:15.947497Z","shell.execute_reply.started":"2022-03-14T22:08:15.879179Z","shell.execute_reply":"2022-03-14T22:08:15.946506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Define the model of Enhanced DF-GAN.</h3>","metadata":{}},{"cell_type":"code","source":"import queue\nimport collections\nimport threading\n\nclass SyncMaster(object):\n    \"\"\"An abstract `SyncMaster` object.\n    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n    and passed to a registered callback.\n    - After receiving the messages, the master device should gather the information and determine to message passed\n    back to each slave devices.\n    \"\"\"\n\n    def __init__(self, master_callback):\n        \"\"\"\n        Args:\n            master_callback: a callback to be invoked after having collected messages from slave devices.\n        \"\"\"\n        self._master_callback = master_callback\n        self._queue = queue.Queue()\n        self._registry = collections.OrderedDict()\n        self._activated = False\n\n    def __getstate__(self):\n        return {'master_callback': self._master_callback}\n\n    def __setstate__(self, state):\n        self.__init__(state['master_callback'])\n\n    def register_slave(self, identifier):\n        \"\"\"\n        Register an slave device.\n        Args:\n            identifier: an identifier, usually is the device id.\n        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n        \"\"\"\n        if self._activated:\n            assert self._queue.empty(), 'Queue is not clean before next initialization.'\n            self._activated = False\n            self._registry.clear()\n        future = FutureResult()\n        self._registry[identifier] = _MasterRegistry(future)\n        return SlavePipe(identifier, self._queue, future)\n\n    def run_master(self, master_msg):\n        \"\"\"\n        Main entry for the master device in each forward pass.\n        The messages were first collected from each devices (including the master device), and then\n        an callback will be invoked to compute the message to be sent back to each devices\n        (including the master device).\n        Args:\n            master_msg: the message that the master want to send to itself. This will be placed as the first\n            message when calling `master_callback`. For detailed usage, see `_SynchronizedBatchNorm` for an example.\n        Returns: the message to be sent back to the master device.\n        \"\"\"\n        self._activated = True\n\n        intermediates = [(0, master_msg)]\n        for i in range(self.nr_slaves):\n            intermediates.append(self._queue.get())\n\n        results = self._master_callback(intermediates)\n        assert results[0][0] == 0, 'The first result should belongs to the master.'\n\n        for i, res in results:\n            if i == 0:\n                continue\n            self._registry[i].result.put(res)\n\n        for i in range(self.nr_slaves):\n            assert self._queue.get() is True\n\n        return results[0][1]\n\n    @property\n    def nr_slaves(self):\n        return len(self._registry)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:08:16.86432Z","iopub.execute_input":"2022-03-14T22:08:16.864646Z","iopub.status.idle":"2022-03-14T22:08:16.876313Z","shell.execute_reply.started":"2022-03-14T22:08:16.864616Z","shell.execute_reply":"2022-03-14T22:08:16.875179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch.nn.modules.batchnorm import _BatchNorm\nfrom torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\n\n__all__ = ['SynchronizedBatchNorm1d', 'SynchronizedBatchNorm2d', 'SynchronizedBatchNorm3d']\n\n\ndef _sum_ft(tensor):\n    \"\"\"sum over the first and last dimention\"\"\"\n    return tensor.sum(dim=0).sum(dim=-1)\n\n\ndef _unsqueeze_ft(tensor):\n    \"\"\"add new dementions at the front and the tail\"\"\"\n    return tensor.unsqueeze(0).unsqueeze(-1)\n\n\n_ChildMessage = collections.namedtuple('_ChildMessage', ['sum', 'ssum', 'sum_size'])\n_MasterMessage = collections.namedtuple('_MasterMessage', ['sum', 'inv_std'])\n\n\nclass _SynchronizedBatchNorm(_BatchNorm):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n\n        self._sync_master = SyncMaster(self._data_parallel_master)\n\n        self._is_parallel = False\n        self._parallel_id = None\n        self._slave_pipe = None\n\n    def forward(self, input):\n        # If it is not parallel computation or is in evaluation mode, use PyTorch's implementation.\n        if not (self._is_parallel and self.training):\n            return F.batch_norm(\n                input, self.running_mean, self.running_var, self.weight, self.bias,\n                self.training, self.momentum, self.eps)\n\n        # Resize the input to (B, C, -1).\n        input_shape = input.size()\n        input = input.view(input.size(0), self.num_features, -1)\n\n        # Compute the sum and square-sum.\n        sum_size = input.size(0) * input.size(2)\n        input_sum = _sum_ft(input)\n        input_ssum = _sum_ft(input ** 2)\n\n        # Reduce-and-broadcast the statistics.\n        if self._parallel_id == 0:\n            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n        else:\n            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n\n        # Compute the output.\n        if self.affine:\n            # MJY:: Fuse the multiplication for speed.\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n        else:\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n\n        # Reshape it.\n        return output.view(input_shape)\n\n    def __data_parallel_replicate__(self, ctx, copy_id):\n        self._is_parallel = True\n        self._parallel_id = copy_id\n\n        # parallel_id == 0 means master device.\n        if self._parallel_id == 0:\n            ctx.sync_master = self._sync_master\n        else:\n            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n\n    def _data_parallel_master(self, intermediates):\n        \"\"\"Reduce the sum and square-sum, compute the statistics, and broadcast it.\"\"\"\n\n        # Always using same \"device order\" makes the ReduceAdd operation faster.\n        # Thanks to:: Tete Xiao (http://tetexiao.com/)\n        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n\n        to_reduce = [i[1][:2] for i in intermediates]\n        to_reduce = [j for i in to_reduce for j in i]  # flatten\n        target_gpus = [i[1].sum.get_device() for i in intermediates]\n\n        sum_size = sum([i[1].sum_size for i in intermediates])\n        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n\n        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n\n        outputs = []\n        for i, rec in enumerate(intermediates):\n            outputs.append((rec[0], _MasterMessage(*broadcasted[i*2:i*2+2])))\n\n        return outputs\n\n    def _compute_mean_std(self, sum_, ssum, size):\n        \"\"\"Compute the mean and standard-deviation with sum and square-sum. This method\n        also maintains the moving average on the master device.\"\"\"\n        assert size > 1, 'BatchNorm computes unbiased standard-deviation, which requires size > 1.'\n        mean = sum_ / size\n        sumvar = ssum - sum_ * mean\n        unbias_var = sumvar / (size - 1)\n        bias_var = sumvar / size\n\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n\n        return mean, bias_var.clamp(self.eps) ** -0.5\n\n\n\n\nclass SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n    def _check_input_dim(self, input):\n        if input.dim() != 4:\n            raise ValueError('expected 4D input (got {}D input)'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:08:18.38557Z","iopub.execute_input":"2022-03-14T22:08:18.386006Z","iopub.status.idle":"2022-03-14T22:08:18.437886Z","shell.execute_reply.started":"2022-03-14T22:08:18.385967Z","shell.execute_reply":"2022-03-14T22:08:18.436863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n#from sync_batchnorm import SynchronizedBatchNorm2d\n\nBatchNorm = SynchronizedBatchNorm2d\n\n\nclass NetG(nn.Module):\n    def __init__(self, ngf=64, nz=100):\n        super(NetG, self).__init__()\n        self.ngf = ngf\n\n        self.conv_mask = nn.Sequential(nn.Conv2d(8 * ngf, 100, 3, 1, 1),\n                                       BatchNorm(100),\n                                       nn.ReLU(),\n                                       nn.Conv2d(100, 1, 1, 1, 0))\n\n        # layer1输入的是一个100x1x1的随机噪声, 输出尺寸(ngf*8)x4x4\n        self.fc = nn.Linear(nz, ngf * 8 * 4 * 4)\n        self.block0 = G_Block(ngf * 8, ngf * 8)  # 4x4\n        self.block1 = G_Block(ngf * 8, ngf * 8)  # 8x8\n        self.block2 = G_Block(ngf * 8, ngf * 8)  # 16x16\n        self.block3 = G_Block(ngf * 8, ngf * 8)  # 32x32\n        self.block4 = G_Block(ngf * 8, ngf * 4)  # 64x64\n        self.block5 = G_Block(ngf * 4, ngf * 2)  # 128x128\n        self.block6 = G_Block(ngf * 2, ngf * 1, predict_mask=False)  # 256x256\n\n        self.conv_img = nn.Sequential(\n            BatchNorm(ngf),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ngf, 3, 3, 1, 1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x, c):\n\n        out = self.fc(x)\n        out = out.view(x.size(0), 8 * self.ngf, 4, 4)\n        hh, ww = out.size(2), out.size(3)\n        stage_mask = self.conv_mask(out)\n        fusion_mask = torch.sigmoid(stage_mask)\n        stage_mask_4 = fusion_mask\n        out, stage_mask = self.block0(out, c, fusion_mask)\n\n        out = F.interpolate(out, scale_factor=2)\n        hh, ww = out.size(2), out.size(3)\n        stage_mask = F.interpolate(stage_mask, size=(hh, ww), mode='bilinear', align_corners=True)\n        fusion_mask = torch.sigmoid(stage_mask)\n        stage_mask_8 = fusion_mask\n        out, stage_mask = self.block1(out, c, fusion_mask)\n\n        out = F.interpolate(out, scale_factor=2)\n        hh, ww = out.size(2), out.size(3)\n        stage_mask = F.interpolate(stage_mask, size=(hh, ww), mode='bilinear', align_corners=True)\n        fusion_mask = torch.sigmoid(stage_mask)\n        stage_mask_16 = fusion_mask\n        out, stage_mask = self.block2(out, c, fusion_mask)\n\n        out = F.interpolate(out, scale_factor=2)\n        hh, ww = out.size(2), out.size(3)\n        stage_mask = F.interpolate(stage_mask, size=(hh, ww), mode='bilinear', align_corners=True)\n        fusion_mask = torch.sigmoid(stage_mask)\n        stage_mask_32 = fusion_mask\n        out, stage_mask = self.block3(out, c, fusion_mask)\n\n        out = F.interpolate(out, scale_factor=2)\n        hh, ww = out.size(2), out.size(3)\n        stage_mask = F.interpolate(stage_mask, size=(hh, ww), mode='bilinear', align_corners=True)\n        fusion_mask = torch.sigmoid(stage_mask)\n        stage_mask_64 = fusion_mask\n        out, stage_mask = self.block4(out, c, fusion_mask)\n\n        out = F.interpolate(out, scale_factor=2)\n        hh, ww = out.size(2), out.size(3)\n        stage_mask = F.interpolate(stage_mask, size=(hh, ww), mode='bilinear', align_corners=True)\n        fusion_mask = torch.sigmoid(stage_mask)\n        stage_mask_128 = fusion_mask\n        out, stage_mask = self.block5(out, c, fusion_mask)\n\n        out = F.interpolate(out, scale_factor=2)\n        hh, ww = out.size(2), out.size(3)\n        stage_mask = F.interpolate(stage_mask, size=(hh, ww), mode='bilinear', align_corners=True)\n        fusion_mask = torch.sigmoid(stage_mask)\n        stage_mask_256 = fusion_mask\n        out, _ = self.block6(out, c, fusion_mask)\n\n        out = self.conv_img(out)\n\n        # return out, fusion_mask\n        return out, [stage_mask_4, stage_mask_8, stage_mask_16, stage_mask_32,\n                     stage_mask_64, stage_mask_128, stage_mask_256]\n\n\nclass G_Block(nn.Module):\n\n    def __init__(self, in_ch, out_ch, num_w=256, predict_mask=True):\n        super(G_Block, self).__init__()\n\n        self.learnable_sc = in_ch != out_ch\n        self.predict_mask = predict_mask\n        self.c1 = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n        self.c2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1)\n        self.affine0 = affine(in_ch)\n        #self.affine1 = affine(in_ch)\n        self.affine2 = affine(out_ch)\n        #self.affine3 = affine(out_ch)\n        self.gamma = nn.Parameter(torch.zeros(1))\n        if self.learnable_sc:\n            self.c_sc = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n\n        if self.predict_mask:\n            self.conv_mask = nn.Sequential(nn.Conv2d(out_ch, 100, 3, 1, 1),\n                                           BatchNorm(100),\n                                           nn.ReLU(),\n                                           nn.Conv2d(100, 1, 1, 1, 0))\n\n    def forward(self, x, y=None, fusion_mask=None):\n        out = self.shortcut(x) + self.gamma * self.residual(x, y, fusion_mask)\n\n        if self.predict_mask:\n            mask = self.conv_mask(out)\n        else:\n            mask = None\n\n        return out, mask\n\n    def shortcut(self, x):\n        if self.learnable_sc:\n            x = self.c_sc(x)\n        return x\n\n    def residual(self, x, y=None, fusion_mask=None):\n        h = self.affine0(x, y, fusion_mask)\n        h = nn.ReLU(inplace=True)(h)\n        h = self.c1(h)\n\n        h = self.affine2(h, y, fusion_mask)\n        h = nn.ReLU(inplace=True)(h)\n        return self.c2(h)\n\n\nclass affine(nn.Module):\n\n    def __init__(self, num_features):\n        super(affine, self).__init__()\n\n        self.batch_norm2d = BatchNorm(num_features, affine=False)\n\n        self.fc_gamma = nn.Sequential(OrderedDict([\n            ('linear1', nn.Linear(256, 256)),\n            ('relu1', nn.ReLU(inplace=True)),\n            ('linear2', nn.Linear(256, num_features)),\n        ]))\n        self.fc_beta = nn.Sequential(OrderedDict([\n            ('linear1', nn.Linear(256, 256)),\n            ('relu1', nn.ReLU(inplace=True)),\n            ('linear2', nn.Linear(256, num_features)),\n        ]))\n        self._initialize()\n\n    def _initialize(self):\n        nn.init.zeros_(self.fc_gamma.linear2.weight.data)\n        nn.init.zeros_(self.fc_gamma.linear2.bias.data)\n        nn.init.zeros_(self.fc_beta.linear2.weight.data)\n        nn.init.zeros_(self.fc_beta.linear2.bias.data)\n\n    def forward(self, x, y=None, fusion_mask=None):\n        x = self.batch_norm2d(x)\n        weight = self.fc_gamma(y)\n        bias = self.fc_beta(y)\n\n        if weight.dim() == 1:\n            weight = weight.unsqueeze(0)\n        if bias.dim() == 1:\n            bias = bias.unsqueeze(0)\n\n        size = x.size()\n        weight = weight.unsqueeze(-1).unsqueeze(-1).expand(size)\n        bias = bias.unsqueeze(-1).unsqueeze(-1).expand(size)\n        weight = weight * fusion_mask + 1\n        bias = bias * fusion_mask\n        return weight * x + bias\n\n\nclass D_GET_LOGITS(nn.Module):\n    def __init__(self, ndf):\n        super(D_GET_LOGITS, self).__init__()\n        self.df_dim = ndf\n\n        self.joint_conv = nn.Sequential(\n            nn.Conv2d(ndf * 16 + 256, ndf * 2, 3, 1, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 2, 1, 4, 1, 0, bias=False),\n        )\n\n    def forward(self, out, y):\n\n        y = y.view(-1, 256, 1, 1)\n        y = y.repeat(1, 1, 4, 4)\n        h_c_code = torch.cat((out, y), 1)\n        out = self.joint_conv(h_c_code)\n        return out\n\n\n# 定义鉴别器网络D\nclass NetD(nn.Module):\n    def __init__(self, ndf):\n        super(NetD, self).__init__()\n\n        self.conv_img = nn.Conv2d(3, ndf, 3, 1, 1)  # 128\n        self.block0 = resD(ndf * 1, ndf * 2)  # 64\n        self.block1 = resD(ndf * 2, ndf * 4)  # 32\n        self.block2 = resD(ndf * 4, ndf * 8)  # 16\n        self.block3 = resD(ndf * 8, ndf * 16)  # 8\n        self.block4 = resD(ndf * 16, ndf * 16)  # 4\n        self.block5 = resD(ndf * 16, ndf * 16)  # 4\n\n        self.COND_DNET = D_GET_LOGITS(ndf)\n\n    def forward(self, x):\n\n        out = self.conv_img(x)\n        out = self.block0(out)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.block4(out)\n        out = self.block5(out)\n\n        return out\n\n\nclass resD(nn.Module):\n    def __init__(self, fin, fout, downsample=True):\n        super().__init__()\n        self.downsample = downsample\n        self.learned_shortcut = (fin != fout)\n        self.conv_r = nn.Sequential(\n            nn.Conv2d(fin, fout, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(fout, fout, 3, 1, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        self.conv_s = nn.Conv2d(fin, fout, 1, stride=1, padding=0)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x, c=None):\n        return self.shortcut(x) + self.gamma * self.residual(x)\n\n    def shortcut(self, x):\n        if self.learned_shortcut:\n            x = self.conv_s(x)\n        if self.downsample:\n            return F.avg_pool2d(x, 2)\n        return x\n\n    def residual(self, x):\n        return self.conv_r(x)\n\n\ndef conv2d(in_feat, out_feat, kernel_size=3, stride=1, padding=1, bias=True, spectral_norm=False):\n    conv = nn.Conv2d(in_feat, out_feat, kernel_size, stride, padding, bias=bias)\n    if spectral_norm:\n        return nn.utils.spectral_norm(conv, eps=1e-4)\n    else:\n        return conv\n\n\ndef linear(in_feat, out_feat, bias=True, spectral_norm=False):\n    lin = nn.Linear(in_feat, out_feat, bias=bias)\n    if spectral_norm:\n        return nn.utils.spectral_norm(lin)\n    else:\n        return lin","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:08:19.285904Z","iopub.execute_input":"2022-03-14T22:08:19.286284Z","iopub.status.idle":"2022-03-14T22:08:19.340353Z","shell.execute_reply.started":"2022-03-14T22:08:19.286245Z","shell.execute_reply":"2022-03-14T22:08:19.339356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_labels(batch_size):\n    # Kai: real_labels and fake_labels have data type: torch.float32\n    # match_labels has data type: torch.int64\n    real_labels = Variable(torch.FloatTensor(batch_size).fill_(1))\n    fake_labels = Variable(torch.FloatTensor(batch_size).fill_(0))\n    match_labels = Variable(torch.LongTensor(range(batch_size)))\n    real_labels = real_labels.cuda()\n    fake_labels = fake_labels.cuda()\n    match_labels = match_labels.cuda()\n    return real_labels, fake_labels, match_labels","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:08:20.255457Z","iopub.execute_input":"2022-03-14T22:08:20.255779Z","iopub.status.idle":"2022-03-14T22:08:20.261304Z","shell.execute_reply.started":"2022-03-14T22:08:20.255749Z","shell.execute_reply":"2022-03-14T22:08:20.260269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import models\ndef conv1x1(in_planes, out_planes, bias=False):\n    \"1x1 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n                     padding=0, bias=bias)\n\ndef func_attention(query, context, gamma1):\n    \"\"\"\n    query: batch x ndf x queryL\n    context: batch x ndf x ih x iw (sourceL=ihxiw)\n    mask: batch_size x sourceL\n    \"\"\"\n    batch_size, queryL = query.size(0), query.size(2)\n    ih, iw = context.size(2), context.size(3)\n    sourceL = ih * iw\n\n    # --> batch x sourceL x ndf\n    context = context.view(batch_size, -1, sourceL)\n    contextT = torch.transpose(context, 1, 2).contiguous()\n\n    # Get attention\n    # (batch x sourceL x ndf)(batch x ndf x queryL)\n    # -->batch x sourceL x queryL\n    attn = torch.bmm(contextT, query)  # Eq. (7) in AttnGAN paper\n    # --> batch*sourceL x queryL\n    attn = attn.cuda()\n    attn = attn.view(batch_size * sourceL, queryL)\n    attn = nn.Softmax()(attn)  # Eq. (8)\n\n    # --> batch x sourceL x queryL\n    attn = attn.view(batch_size, sourceL, queryL)\n    # --> batch*queryL x sourceL\n    attn = torch.transpose(attn, 1, 2).contiguous()\n    attn = attn.view(batch_size * queryL, sourceL)\n    #  Eq. (9)\n    attn = attn * gamma1\n    attn = nn.Softmax()(attn)\n    attn = attn.view(batch_size, queryL, sourceL)\n    # --> batch x sourceL x queryL\n    attnT = torch.transpose(attn, 1, 2).contiguous()\n\n    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n    # --> batch x ndf x queryL\n    weightedContext = torch.bmm(context, attnT)\n\n    return weightedContext, attn.view(batch_size, -1, ih, iw)\n\nclass CNN_ENCODER(nn.Module):\n    def __init__(self, nef):\n        super(CNN_ENCODER, self).__init__()\n        if True:\n            self.nef = nef\n        else:\n            self.nef = 256  # define a uniform ranker\n\n        #model = models.inception_v3()\n        #url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n        # model.load_state_dict(model_zoo.load_url(url))\n        model = models.inception_v3(pretrained=True, transform_input=False)\n        for param in model.parameters():\n            param.requires_grad = False\n        #print('Load pretrained model from ', url)\n        # print(model)\n\n        print('Load pretrained inception v3 model')\n        self.define_module(model)\n        self.init_trainable_weights()\n\n    def define_module(self, model):\n        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n        self.Mixed_5b = model.Mixed_5b\n        self.Mixed_5c = model.Mixed_5c\n        self.Mixed_5d = model.Mixed_5d\n        self.Mixed_6a = model.Mixed_6a\n        self.Mixed_6b = model.Mixed_6b\n        self.Mixed_6c = model.Mixed_6c\n        self.Mixed_6d = model.Mixed_6d\n        self.Mixed_6e = model.Mixed_6e\n        self.Mixed_7a = model.Mixed_7a\n        self.Mixed_7b = model.Mixed_7b\n        self.Mixed_7c = model.Mixed_7c\n\n        self.emb_features = conv1x1(768, self.nef)\n        self.emb_cnn_code = nn.Linear(2048, self.nef)\n\n    def init_trainable_weights(self):\n        initrange = 0.1\n        self.emb_features.weight.data.uniform_(-initrange, initrange)\n        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, x):\n        features = None\n        # --> fixed-size input: batch x 3 x 299 x 299\n        x = nn.functional.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)\n        # 299 x 299 x 3\n        x = self.Conv2d_1a_3x3(x)\n        # 149 x 149 x 32\n        x = self.Conv2d_2a_3x3(x)\n        # 147 x 147 x 32\n        x = self.Conv2d_2b_3x3(x)\n        # 147 x 147 x 64\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # 73 x 73 x 64\n        x = self.Conv2d_3b_1x1(x)\n        # 73 x 73 x 80\n        x = self.Conv2d_4a_3x3(x)\n        # 71 x 71 x 192\n\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # 35 x 35 x 192\n        x = self.Mixed_5b(x)\n        # 35 x 35 x 256\n        x = self.Mixed_5c(x)\n        # 35 x 35 x 288\n        x = self.Mixed_5d(x)\n        # 35 x 35 x 288\n\n        x = self.Mixed_6a(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6b(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6c(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6d(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6e(x)\n        # 17 x 17 x 768\n\n        # image region features\n        features = x\n        # 17 x 17 x 768\n\n        x = self.Mixed_7a(x)\n        # 8 x 8 x 1280\n        x = self.Mixed_7b(x)\n        # 8 x 8 x 2048\n        x = self.Mixed_7c(x)\n        # 8 x 8 x 2048\n        x = F.avg_pool2d(x, kernel_size=8)\n        # 1 x 1 x 2048\n        # x = F.dropout(x, training=self.training)\n        # 1 x 1 x 2048\n        x = x.view(x.size(0), -1)\n        # 2048\n\n        # global image features\n        cnn_code = self.emb_cnn_code(x)\n        # 512\n        if features is not None:\n            features = self.emb_features(features)\n        return features, cnn_code","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:08:21.452376Z","iopub.execute_input":"2022-03-14T22:08:21.452698Z","iopub.status.idle":"2022-03-14T22:08:21.474007Z","shell.execute_reply.started":"2022-03-14T22:08:21.452667Z","shell.execute_reply":"2022-03-14T22:08:21.472926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Calculate the cosine similarity, this function will be used in the evaluation </h3>","metadata":{"execution":{"iopub.status.busy":"2024-03-03T19:45:05.151944Z","iopub.execute_input":"2024-03-03T19:45:05.152346Z","iopub.status.idle":"2024-03-03T19:45:05.157658Z","shell.execute_reply.started":"2024-03-03T19:45:05.152314Z","shell.execute_reply":"2024-03-03T19:45:05.156494Z"}}},{"cell_type":"code","source":"def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n    \"\"\"Returns cosine similarity between x1 and x2, computed along dim.\n    \"\"\"\n    w12 = torch.sum(x1 * x2, dim)\n    w1 = torch.norm(x1, 2, dim)\n    w2 = torch.norm(x2, 2, dim)\n    return (w12 / (w1 * w2).clamp(min=eps)).squeeze()\n\n\ndef words_loss(img_features, words_emb, labels,\n               cap_lens, class_ids, batch_size):\n    \"\"\"\n        words_emb(query): batch x nef x seq_len\n        img_features(context): batch x nef x 17 x 17\n    \"\"\"\n    masks = []\n    att_maps = []\n    similarities = []\n    cap_lens = cap_lens.data.tolist()\n    for i in range(batch_size):\n        if class_ids is not None:\n            mask = (class_ids == class_ids[i])\n            mask[i] = 0\n            masks.append(mask.view(1, -1))\n        # Get the i-th text description\n        words_num = cap_lens[i]\n        # -> 1 x nef x words_num\n        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n        # -> batch_size x nef x words_num\n        word = word.repeat(batch_size, 1, 1)\n        # batch x nef x 17*17\n        context = img_features\n        \"\"\"\n            word(query): batch x nef x words_num\n            context: batch x nef x 17 x 17\n            weiContext: batch x nef x words_num\n            attn: batch x words_num x 17 x 17\n        \"\"\"\n        weiContext, attn = func_attention(word.cuda(), context.cuda(), 5.0)\n        att_maps.append(attn[i].unsqueeze(0).contiguous())\n        # --> batch_size x words_num x nef\n        word = word.transpose(1, 2).contiguous()\n        weiContext = weiContext.transpose(1, 2).contiguous()\n        # --> batch_size*words_num x nef\n        word = word.view(batch_size * words_num, -1)\n        weiContext = weiContext.view(batch_size * words_num, -1)\n        #\n        # -->batch_size*words_num\n        row_sim = cosine_similarity(word, weiContext)\n        # --> batch_size x words_num\n        row_sim = row_sim.view(batch_size, words_num)\n\n        # Eq. (10)\n        row_sim.mul_(5.0).exp_()\n        row_sim = row_sim.sum(dim=1, keepdim=True)\n        row_sim = torch.log(row_sim)\n\n        # --> 1 x batch_size\n        # similarities(i, j): the similarity between the i-th image and the j-th text description\n        similarities.append(row_sim)\n\n    # batch_size x batch_size\n    similarities = torch.cat(similarities, 1)\n    if class_ids is not None:\n        # masks: batch_size x batch_size\n        masks = torch.cat(masks, dim=0)\n        masks = masks.cuda()\n    similarities = similarities * 10.0\n    if class_ids is not None:\n        similarities.data.masked_fill_(masks, -float('inf'))\n    similarities1 = similarities.transpose(0, 1)\n    if labels is not None:\n        loss0 = nn.CrossEntropyLoss()(similarities, labels)\n        loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n    else:\n        loss0, loss1 = None, None\n    return loss0, loss1, att_maps\n\n\n# ##################Loss for G and Ds##############################\n\n\n\ndef DAMSM_loss(image_encoder, fake_imgs, real_labels,\n               words_embs, sent_emb, match_labels,\n               cap_lens, class_ids):\n    class_ids = torch.LongTensor(class_ids)\n    batch_size = real_labels.size(0)\n    # Forward\n\n    # words_features: batch_size x nef x 17 x 17\n    # sent_code: batch_size x nef\n    region_features, cnn_code = image_encoder(fake_imgs)\n    w_loss0, w_loss1, _ = words_loss(region_features, words_embs,\n                                     match_labels, cap_lens,\n                                     class_ids, batch_size)\n    w_loss = (w_loss0 + w_loss1) * \\\n        1.0\n    # err_words = err_words + w_loss.data[0]\n\n    # s_loss0, s_loss1 = sent_loss(cnn_code, sent_emb,\n    #                             match_labels, class_ids, batch_size)\n    # s_loss = (s_loss0 + s_loss1) * \\\n    #    cfg.TRAIN.SMOOTH.LAMBDA\n    # err_sent = err_sent + s_loss.data[0]\n\n    #DAMSM = w_loss + s_loss\n\n    DAMSM = w_loss\n    return DAMSM","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:08:22.365178Z","iopub.execute_input":"2022-03-14T22:08:22.3655Z","iopub.status.idle":"2022-03-14T22:08:22.382123Z","shell.execute_reply.started":"2022-03-14T22:08:22.365472Z","shell.execute_reply":"2022-03-14T22:08:22.381056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Define the model, you don't have to upload some pre-trained model in the first, I upload them because a break dowen the training process</h3>","metadata":{}},{"cell_type":"code","source":"image_encoder = CNN_ENCODER(256)\n#image_encoder_optimazer = torch.optim.Adam(image_encoder.parameters(), lr=0.00004, betas=(0.0, 0.9))\n#image_encoder.cuda()\nencoder_parameters = list(text_encoder.parameters())\nfor v in image_encoder.parameters():\n    encoder_parameters.append(v)\noptimizerEncoder = torch.optim.Adam(encoder_parameters, lr=0.00004, betas=(0.0, 0.9))\nimage_encoder.cuda()\ntext_encoder.cuda()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:08:23.56626Z","iopub.execute_input":"2022-03-14T22:08:23.566669Z","iopub.status.idle":"2022-03-14T22:08:32.674493Z","shell.execute_reply.started":"2022-03-14T22:08:23.566631Z","shell.execute_reply":"2022-03-14T22:08:32.673668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### device = \"cuda\"\nimport torchvision.utils as vutils\nimport random\nbatch_size = 32\nrandom.seed(100)\nnp.random.seed(100)\ntorch.manual_seed(100)\ntorch.cuda.manual_seed_all(100)\nCUDA_LAUNCH_BLOCKING=1\nnetG = NetG(batch_size, 100).to(device)\nnetD = NetD(batch_size).to(device)\noptimizerG = torch.optim.Adam(netG.parameters(), lr=0.0001, betas=(0.0, 0.9))\noptimizerD = torch.optim.Adam(netD.parameters(), lr=0.0004, betas=(0.0, 0.9))\ncheckpoint_G = torch.load('../input/f-modul-ssa-dam-24/netG_Flower_vSSA_DAMSM_30.pt')\nnetG.load_state_dict(checkpoint_G['model_state_dict'])\noptimizerG.load_state_dict(checkpoint_G['optimizer_state_dict'])\noptimizerEncoder.load_state_dict(checkpoint_G['optimizerEncoder'])\ntext_encoder.load_state_dict(checkpoint_G['text_encoder'])\nimage_encoder.load_state_dict(checkpoint_G['image_encoder'])\nepoch_ = checkpoint_G['epoch']\n\ncheckpoint_D = torch.load('../input/f-modul-ssa-dam-24/netD_Flower_vSSA_DAMSM_30.pt')\nnetD.load_state_dict(checkpoint_D['model_state_dict'])\noptimizerD.load_state_dict(checkpoint_D['optimizer_state_dict'])\nnetG.train()\nnetD.train()\nimage_encoder.train()\ntext_encoder.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:09:13.874984Z","iopub.execute_input":"2022-03-14T22:09:13.875334Z","iopub.status.idle":"2022-03-14T22:09:18.74422Z","shell.execute_reply.started":"2022-03-14T22:09:13.875302Z","shell.execute_reply":"2022-03-14T22:09:18.743367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>The training loop, and the save of the model each 2 epochs.</h3>","metadata":{"execution":{"iopub.status.busy":"2024-03-03T19:48:19.079499Z","iopub.execute_input":"2024-03-03T19:48:19.079873Z","iopub.status.idle":"2024-03-03T19:48:19.085424Z","shell.execute_reply.started":"2024-03-03T19:48:19.079838Z","shell.execute_reply":"2024-03-03T19:48:19.084250Z"}}},{"cell_type":"code","source":"\nfor epoch in range(epoch_ +1,epoch_ +41):\n    i = 0\n    for images,captions,cap_lens,class_ids in train_loader:\n        cap_lens,index = torch.sort(cap_lens,descending = True)\n        captions = captions[index]\n        images = images[index]\n        class_ids = class_ids[index]\n        real_labels, fake_labels, match_labels = prepare_labels(batch_size)\n        hidden = text_encoder.init_hidden(batch_size)\n            # words_embs: batch_size x nef x seq_len\n            # sent_emb: batch_size x nef\n        words_embs, sent_emb = text_encoder(captions.to(device), cap_lens.to(device), hidden)\n        words_embs_de, sent_emb_de = words_embs.detach().to(device), sent_emb.detach().to(device)\n        imgs = images.to(device)\n        real_features = netD(imgs.to(device))\n        output = netD.COND_DNET(real_features.to(device), sent_emb_de.to(device))\n        errD_real = torch.nn.ReLU()(1.0 - output).mean()\n\n        output = netD.COND_DNET(real_features[:(batch_size - 1)].to(device), sent_emb_de[1:batch_size].to(device))\n        errD_mismatch = torch.nn.ReLU()(1.0 + output).mean()\n\n            # synthesize fake images\n        noise = torch.randn(batch_size, 100)\n        noise = noise.to(device)\n        fake, _ = netG(noise, sent_emb_de.to(device))\n\n            # update encoder\n        DAMSM_D = DAMSM_loss(image_encoder, images.to(device), real_labels, words_embs.to(device),\n                                 sent_emb.to(device), match_labels, cap_lens.to(device), class_ids)\n        optimizerEncoder.zero_grad()\n        DAMSM_D.backward()\n        optimizerEncoder.step()\n\n            # G does not need update with D\n        fake_features = netD(fake.detach())\n\n        errD_fake = netD.COND_DNET(fake_features.to(device), sent_emb_de.to(device))\n        errD_fake = torch.nn.ReLU()(1.0 + errD_fake).mean()\n\n        errD = errD_real + (errD_fake + errD_mismatch) / 2.0\n        optimizerD.zero_grad()\n        errD.backward()\n        optimizerD.step()\n\n            # MA-GP\n        interpolated = (imgs.data).requires_grad_()\n        sent_inter = (sent_emb_de.data).requires_grad_()\n        features = netD(interpolated.to(device))\n        out = netD.COND_DNET(features, sent_inter)\n        grads = torch.autograd.grad(outputs=out,\n                                        inputs=(interpolated, sent_inter),\n                                        grad_outputs=torch.ones(out.size()).cuda(),\n                                        retain_graph=True,\n                                        create_graph=True,\n                                        only_inputs=True)\n        grad0 = grads[0].view(grads[0].size(0), -1).to(device)\n        grad1 = grads[1].view(grads[1].size(0), -1).to(device)\n        grad = torch.cat((grad0, grad1), dim=1)\n        grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n        d_loss_gp = torch.mean((grad_l2norm) ** 6)\n        d_loss = 2.0 * d_loss_gp\n        optimizerD.zero_grad()\n        d_loss.backward()\n        optimizerD.step()\n\n            # update G\n        features = netD(fake)\n        output = netD.COND_DNET(features.to(device), sent_emb_de.to(device))\n        errG = - output.mean()\n        DAMSM_G = 0.1 * DAMSM_loss(image_encoder, fake.to(device), real_labels, words_embs_de.to(device),\n                                       sent_emb_de.to(device), match_labels, cap_lens.to(device), class_ids)\n        errG_total = errG + DAMSM_G\n        optimizerG.zero_grad()\n        errG_total.backward()\n        optimizerG.step()\n        print('[%d/%d][%d/%d] Loss_D: %.3f Loss_G %.3f DAMSM_D %.3f DAMSM_G %.3f'\n                % (epoch, 600, i, len(train_loader), errD.item(), errG.item(),DAMSM_D.item(),DAMSM_G.item()))\n        i +=1\n        if epoch%2 == 0 and i == 224:\n            print(\"save ................\")\n            vutils.save_image(fake.data,\n                        '%s/fake_samples_epoch_%03d.png' % ('./', epoch+1),\n                        normalize=True)\n            torch.save({\n            'model_state_dict': netD.state_dict(),\n            'optimizer_state_dict': optimizerD.state_dict(),\n            },'./netD_Flower_vSSA_DAMSM_54.pt')\n            \n            torch.save({\n            'epoch': epoch,\n            'model_state_dict': netG.state_dict(),\n            'optimizer_state_dict': optimizerG.state_dict(),\n            'text_encoder':text_encoder.state_dict(),\n            'optimizerEncoder': optimizerEncoder.state_dict(),\n            'image_encoder':image_encoder.state_dict(),\n            },'./netG_Flower_vSSA_DAMSM_54.pt')","metadata":{"execution":{"iopub.status.busy":"2022-03-14T22:12:20.414116Z","iopub.execute_input":"2022-03-14T22:12:20.414452Z","iopub.status.idle":"2022-03-15T09:01:24.780492Z","shell.execute_reply.started":"2022-03-14T22:12:20.414422Z","shell.execute_reply":"2022-03-15T09:01:24.777965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"            print(\"save ................\")\n            vutils.save_image(fake.data,\n                        '%s/fake_samples_epoch_%03d.png' % ('./', epoch+1),\n                        normalize=True)\n            torch.save({\n            'model_state_dict': netD.state_dict(),\n            'optimizer_state_dict': optimizerD.state_dict(),\n            },'./netD_Flower_vSSA_DAMSM_30.pt')\n            \n            torch.save({\n            'epoch': epoch,\n            'model_state_dict': netG.state_dict(),\n            'optimizer_state_dict': optimizerG.state_dict(),\n            'text_encoder':text_encoder.state_dict(),\n            'optimizerEncoder': optimizerEncoder.state_dict(),\n            'image_encoder':image_encoder.state_dict(),\n            },'./netG_Flower_vSSA_DAMSM_30.pt')","metadata":{"execution":{"iopub.status.busy":"2022-03-14T09:14:12.462761Z","iopub.execute_input":"2022-03-14T09:14:12.463123Z","iopub.status.idle":"2022-03-14T09:14:14.314081Z","shell.execute_reply.started":"2022-03-14T09:14:12.463094Z","shell.execute_reply":"2022-03-14T09:14:14.313262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### os.mkdir('./Output/')","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:47:29.151829Z","iopub.execute_input":"2022-02-13T10:47:29.152089Z","iopub.status.idle":"2022-02-13T10:47:29.156969Z","shell.execute_reply.started":"2022-02-13T10:47:29.152057Z","shell.execute_reply":"2022-02-13T10:47:29.156283Z"}}},{"cell_type":"markdown","source":"<h3>The evaluation of the model using the test set</h3>","metadata":{}},{"cell_type":"code","source":"#os.mkdir('./Output/')\nbatch_size = 32\nsave_dir = './Output/'\nd = 0\nt = 0\nfor i in range(1):  # (cfg.TEXT.CAPTIONS_PER_IMAGE):\n    for images,captions,cap_lens in test_loader:\n        cap_lens,index = torch.sort(cap_lens,descending = True)\n        captions = captions[index]\n        images = images[index]\n        with torch.no_grad():\n            hidden = text_encoder.init_hidden(batch_size)\n            words_embs, sent_emb = text_encoder(captions.to(device), cap_lens.to(device), hidden)\n            words_embs, sent_emb = words_embs.detach().to(device), sent_emb.detach().to(device)\n            #sent_emb = sent_emb.transpose(0, 1).contiguous()\n            \n        #print(\"captions :\",captions.size())\n        #print(\"captions :\",captions[3])\n        #print(\"images :\",images.size())\n        #words_embs, sent_emb = text_encoder(captions.to(device),cap_lens.to(device),hidden)\n        #words_embs, sent_emb = words_embs.detach().to(device), sent_emb.detach().to(device)\n            #######################################################\n            # (2) Generate fake images\n            ######################################################\n        with torch.no_grad():\n            noise = torch.randn(batch_size, 100)\n            noise=noise.to(device)\n            fake_imgs,_ = netG(noise,sent_emb)\n        print(\">>>>>>>>batch : \",d)\n        d += 1\n        for j in range(batch_size):\n            s_tmp = './Output/'\n            im = fake_imgs[j].data.cpu().numpy()\n                # [-1, 1] --> [0, 255]\n            #arab_cap = captions[j]\n            im = (im + 1.0) * 127.5\n            im = im.astype(np.uint8)\n            im = np.transpose(im, (1, 2, 0))\n            im = Image.fromarray(im)\n            #arab_cap = tokenizer.decode(arab_cap)\n            #cap_path = '%s_%3d.txt' % (s_tmp,t)\n            fullpath = '%s_%3d.png' % (s_tmp,t)\n            t += 1\n            im.save(fullpath)\n           # with open(cap_path,'w') as f:\n           #     f.write(arab_cap)\n                ","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:53:21.015092Z","iopub.execute_input":"2022-02-13T10:53:21.0158Z","iopub.status.idle":"2022-02-13T10:55:29.741176Z","shell.execute_reply.started":"2022-02-13T10:53:21.015762Z","shell.execute_reply":"2022-02-13T10:55:29.740432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r Output.zip ./Output","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:55:35.24842Z","iopub.execute_input":"2022-02-13T10:55:35.248991Z","iopub.status.idle":"2022-02-13T10:55:43.050396Z","shell.execute_reply.started":"2022-02-13T10:55:35.248951Z","shell.execute_reply":"2022-02-13T10:55:43.049621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>The use of FID and IS to evaluate the Quality of the generated images</h3>","metadata":{"execution":{"iopub.status.busy":"2024-03-03T19:50:08.796473Z","iopub.execute_input":"2024-03-03T19:50:08.796862Z","iopub.status.idle":"2024-03-03T19:50:08.802356Z","shell.execute_reply.started":"2024-03-03T19:50:08.796825Z","shell.execute_reply":"2024-03-03T19:50:08.801202Z"}}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\n\nclass InceptionV3(nn.Module):\n    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n\n    # Index of default block of inception to return,\n    # corresponds to output of final average pooling\n    DEFAULT_BLOCK_INDEX = 3\n\n    # Maps feature dimensionality to their output blocks indices\n    BLOCK_INDEX_BY_DIM = {\n        64: 0,   # First max pooling features\n        192: 1,  # Second max pooling featurs\n        768: 2,  # Pre-aux classifier features\n        2048: 3  # Final average pooling features\n    }\n\n    def __init__(self,\n                 output_blocks=[DEFAULT_BLOCK_INDEX],\n                 resize_input=True,\n                 normalize_input=True,\n                 requires_grad=False):\n        \"\"\"Build pretrained InceptionV3\n        Parameters\n        ----------\n        output_blocks : list of int\n            Indices of blocks to return features of. Possible values are:\n                - 0: corresponds to output of first max pooling\n                - 1: corresponds to output of second max pooling\n                - 2: corresponds to output which is fed to aux classifier\n                - 3: corresponds to output of final average pooling\n        resize_input : bool\n            If true, bilinearly resizes input to width and height 299 before\n            feeding input to model. As the network without fully connected\n            layers is fully convolutional, it should be able to handle inputs\n            of arbitrary size, so resizing might not be strictly needed\n        normalize_input : bool\n            If true, normalizes the input to the statistics the pretrained\n            Inception network expects\n        requires_grad : bool\n            If true, parameters of the model require gradient. Possibly useful\n            for finetuning the network\n        \"\"\"\n        super(InceptionV3, self).__init__()\n\n        self.resize_input = resize_input\n        self.normalize_input = normalize_input\n        self.output_blocks = sorted(output_blocks)\n        self.last_needed_block = max(output_blocks)\n\n        assert self.last_needed_block <= 3, \\\n            'Last possible output block index is 3'\n\n        self.blocks = nn.ModuleList()\n\n        inception = models.inception_v3(pretrained=True)\n\n        # Block 0: input to maxpool1\n        block0 = [\n            inception.Conv2d_1a_3x3,\n            inception.Conv2d_2a_3x3,\n            inception.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        ]\n        self.blocks.append(nn.Sequential(*block0))\n\n        # Block 1: maxpool1 to maxpool2\n        if self.last_needed_block >= 1:\n            block1 = [\n                inception.Conv2d_3b_1x1,\n                inception.Conv2d_4a_3x3,\n                nn.MaxPool2d(kernel_size=3, stride=2)\n            ]\n            self.blocks.append(nn.Sequential(*block1))\n\n        # Block 2: maxpool2 to aux classifier\n        if self.last_needed_block >= 2:\n            block2 = [\n                inception.Mixed_5b,\n                inception.Mixed_5c,\n                inception.Mixed_5d,\n                inception.Mixed_6a,\n                inception.Mixed_6b,\n                inception.Mixed_6c,\n                inception.Mixed_6d,\n                inception.Mixed_6e,\n            ]\n            self.blocks.append(nn.Sequential(*block2))\n\n        # Block 3: aux classifier to final avgpool\n        if self.last_needed_block >= 3:\n            block3 = [\n                inception.Mixed_7a,\n                inception.Mixed_7b,\n                inception.Mixed_7c,\n                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n            ]\n            self.blocks.append(nn.Sequential(*block3))\n\n        for param in self.parameters():\n            param.requires_grad = requires_grad\n\n    def forward(self, inp):\n        \"\"\"Get Inception feature maps\n        Parameters\n        ----------\n        inp : torch.autograd.Variable\n            Input tensor of shape Bx3xHxW. Values are expected to be in \n            range (0, 1)\n        Returns\n        -------\n        List of torch.autograd.Variable, corresponding to the selected output \n        block, sorted ascending by index\n        \"\"\"\n        outp = []\n        x = inp\n\n        if self.resize_input:\n            x = F.upsample(x, size=(299, 299), mode='bilinear', align_corners=True)\n\n        if self.normalize_input:\n            x = x.clone()\n            x[:, 0] = x[:, 0] * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n            x[:, 1] = x[:, 1] * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n            x[:, 2] = x[:, 2] * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n\n        for idx, block in enumerate(self.blocks):\n            x = block(x)\n            if idx in self.output_blocks:\n                outp.append(x)\n\n            if idx == self.last_needed_block:\n                break\n\n        return outp","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:56:02.304614Z","iopub.execute_input":"2022-02-13T10:56:02.304883Z","iopub.status.idle":"2022-02-13T10:56:02.323844Z","shell.execute_reply.started":"2022-02-13T10:56:02.304853Z","shell.execute_reply":"2022-02-13T10:56:02.323069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\nmodel_incp = InceptionV3([block_idx])\nmodel_incp=model_incp.cuda()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:56:04.752701Z","iopub.execute_input":"2022-02-13T10:56:04.753407Z","iopub.status.idle":"2022-02-13T10:56:07.720307Z","shell.execute_reply.started":"2022-02-13T10:56:04.753366Z","shell.execute_reply":"2022-02-13T10:56:07.71955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_activation_statistics(images,model,batch_size=128, dims=2048,\n                    cuda=False):\n    model.eval()\n    act=np.empty((len(images), dims))\n    \n    if cuda:\n        batch=images.cuda()\n    else:\n        batch=images\n    pred = model(batch)[0]\n\n        # If model output is not scalar, apply global spatial average pooling.\n        # This happens if you choose a dimensionality not equal 2048.\n    if pred.size(2) != 1 or pred.size(3) != 1:\n        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n\n    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n    \n    mu = np.mean(act, axis=0)\n    sigma = np.cov(act, rowvar=False)\n    return mu, sigma","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:56:11.473648Z","iopub.execute_input":"2022-02-13T10:56:11.473899Z","iopub.status.idle":"2022-02-13T10:56:11.480923Z","shell.execute_reply.started":"2022-02-13T10:56:11.473871Z","shell.execute_reply":"2022-02-13T10:56:11.480096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n    \"\"\"\n\n    mu1 = np.atleast_1d(mu1)\n    mu2 = np.atleast_1d(mu2)\n\n    sigma1 = np.atleast_2d(sigma1)\n    sigma2 = np.atleast_2d(sigma2)\n\n    assert mu1.shape == mu2.shape, \\\n        'Training and test mean vectors have different lengths'\n    assert sigma1.shape == sigma2.shape, \\\n        'Training and test covariances have different dimensions'\n\n    diff = mu1 - mu2\n\n    \n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    if not np.isfinite(covmean).all():\n        msg = ('fid calculation produces singular product; '\n               'adding %s to diagonal of cov estimates') % eps\n        print(msg)\n        offset = np.eye(sigma1.shape[0]) * eps\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n    \n    if np.iscomplexobj(covmean):\n        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n            m = np.max(np.abs(covmean.imag))\n            raise ValueError('Imaginary component {}'.format(m))\n        covmean = covmean.real\n\n    tr_covmean = np.trace(covmean)\n\n    return (diff.dot(diff) + np.trace(sigma1) +\n            np.trace(sigma2) - 2 * tr_covmean)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T08:38:32.223073Z","iopub.execute_input":"2022-03-13T08:38:32.223768Z","iopub.status.idle":"2022-03-13T08:38:32.237586Z","shell.execute_reply.started":"2022-03-13T08:38:32.223724Z","shell.execute_reply":"2022-03-13T08:38:32.23682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_fretchet(images_real,images_fake,model):\n    mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n    mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n    fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n    return fid_value","metadata":{"execution":{"iopub.status.busy":"2022-03-13T08:38:34.842447Z","iopub.execute_input":"2022-03-13T08:38:34.842905Z","iopub.status.idle":"2022-03-13T08:38:34.848785Z","shell.execute_reply.started":"2022-03-13T08:38:34.842864Z","shell.execute_reply":"2022-03-13T08:38:34.8478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"./Output/\"\noutput_images = []\nwith os.scandir('./Output') as entrie:\n    for entry in entrie:\n        output_images.append(path+entry.name)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T08:38:38.28406Z","iopub.execute_input":"2022-03-13T08:38:38.284458Z","iopub.status.idle":"2022-03-13T08:38:38.304404Z","shell.execute_reply.started":"2022-03-13T08:38:38.284421Z","shell.execute_reply":"2022-03-13T08:38:38.303118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, 'rb') as f:\n        img = Image.open(f)\n        return img.convert('RGB')\n    \n        \ntrain_transforms =  transforms.Compose([\n                                 transforms.Scale(32),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n                             ])\n\nclass dataset(torch.utils.data.Dataset):\n\n    def __init__(self,images_paths,transform=None):\n        self.images_paths = images_paths \n        self.transform = transform \n         \n    #dataset length\n    def __len__(self):\n        self.filelength = len(self.images_paths)\n        return self.filelength\n    \n    #load an one of images\n    def __getitem__(self,idx):\n     #   img_path = self.file_list[idx]\n        img = pil_loader(self.images_paths[idx].strip())\n        img_transformed = self.transform(img)   \n        #img_transformed = img_transformed.expand(3,256,256)\n        return img_transformed","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:57:19.304533Z","iopub.execute_input":"2022-02-13T10:57:19.304801Z","iopub.status.idle":"2022-02-13T10:57:19.315778Z","shell.execute_reply.started":"2022-02-13T10:57:19.304771Z","shell.execute_reply":"2022-02-13T10:57:19.314997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_images = dataset(output_images,transform=train_transforms)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:57:22.400109Z","iopub.execute_input":"2022-02-13T10:57:22.401014Z","iopub.status.idle":"2022-02-13T10:57:22.405902Z","shell.execute_reply.started":"2022-02-13T10:57:22.400963Z","shell.execute_reply":"2022-02-13T10:57:22.404835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nimport torch.utils.data\n\nfrom torchvision.models.inception import inception_v3\n\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef inception_score(imgs, cuda=True, batch_size=32, resize=True, splits=1):\n    \"\"\"Computes the inception score of the generated images imgs\n    imgs -- Torch dataset of (3xHxW) numpy images normalized in the range [-1, 1]\n    cuda -- whether or not to run on GPU\n    batch_size -- batch size for feeding into Inception v3\n    splits -- number of splits\n    \"\"\"\n    N = len(imgs)\n\n    assert batch_size > 0\n    assert N > batch_size\n\n    # Set up dtype\n    if cuda:\n        dtype = torch.cuda.FloatTensor\n    else:\n        if torch.cuda.is_available():\n            print(\"WARNING: You have a CUDA device, so you should probably set cuda=True\")\n        dtype = torch.FloatTensor\n\n    # Set up dataloader\n    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n\n    # Load inception model\n    inception_model = inception_v3(pretrained=True, transform_input=False).type(dtype)\n    inception_model.eval();\n    up = nn.Upsample(size=(299, 299), mode='bilinear').type(dtype)\n    def get_pred(x):\n        if resize:\n            x = up(x)\n        x = inception_model(x)\n        return F.softmax(x).data.cpu().numpy()\n\n    # Get predictions\n    preds = np.zeros((N, 1000))\n\n    for i, batch in enumerate(dataloader, 0):\n        batch = batch.type(dtype)\n        batchv = Variable(batch)\n        batch_size_i = batch.size()[0]\n\n        preds[i*batch_size:i*batch_size + batch_size_i] = get_pred(batchv)\n\n    # Now compute the mean kl-div\n    split_scores = []\n\n    for k in range(splits):\n        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n        py = np.mean(part, axis=0)\n        scores = []\n        for i in range(part.shape[0]):\n            pyx = part[i, :]\n            scores.append(entropy(pyx, py))\n        split_scores.append(np.exp(np.mean(scores)))\n\n    return np.mean(split_scores), np.std(split_scores)\n\n\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\n\nprint (\"Calculating Inception Score...\")\nprint (inception_score(generated_images, cuda=True, batch_size=32, resize=True, splits=10))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:57:35.959604Z","iopub.execute_input":"2022-02-13T10:57:35.960357Z","iopub.status.idle":"2022-02-13T10:57:51.23874Z","shell.execute_reply.started":"2022-02-13T10:57:35.960319Z","shell.execute_reply":"2022-02-13T10:57:51.237993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import linalg\nfrom PIL import Image\nbatch_size = 32\nsave_dir = './Output/'\nfrech = []\nd = 0\nt = 0\n    \nfor i in range(1):  # (cfg.TEXT.CAPTIONS_PER_IMAGE):\n    for images,captions,cap_lens in test_loader:\n        cap_lens,index = torch.sort(cap_lens,descending = True)\n        captions = captions[index]\n        images = images[index]\n        with torch.no_grad():\n            hidden = text_encoder.init_hidden(batch_size)\n            words_embs, sent_emb = text_encoder(captions.to(device), cap_lens.to(device), hidden)\n            words_embs, sent_emb = words_embs.detach().to(device), sent_emb.detach().to(device)\n            #sent_emb = sent_emb.transpose(0, 1).contiguous()\n            \n        #print(\"captions :\",captions.size())\n        #print(\"captions :\",captions[3])\n        #print(\"images :\",images.size())\n        #words_embs, sent_emb = text_encoder(captions.to(device),cap_lens.to(device),hidden)\n        #words_embs, sent_emb = words_embs.detach().to(device), sent_emb.detach().to(device)\n            #######################################################\n            # (2) Generate fake images\n            ######################################################\n        with torch.no_grad():\n            noise = torch.randn(batch_size, 100)\n            noise=noise.to(device)\n            fake_imgs,_ = netG(noise,sent_emb)\n        fd = calculate_fretchet(images,fake_imgs,model_incp)\n        print(fd)\n        frech.append(fd)\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-13T11:00:08.735562Z","iopub.execute_input":"2022-02-13T11:00:08.736097Z","iopub.status.idle":"2022-02-13T11:02:20.170997Z","shell.execute_reply.started":"2022-02-13T11:00:08.736039Z","shell.execute_reply":"2022-02-13T11:02:20.168488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r ./output.zip ./Output","metadata":{"execution":{"iopub.status.busy":"2021-09-04T19:20:08.832673Z","iopub.execute_input":"2021-09-04T19:20:08.833018Z","iopub.status.idle":"2021-09-04T19:20:09.51403Z","shell.execute_reply.started":"2021-09-04T19:20:08.832987Z","shell.execute_reply":"2021-09-04T19:20:09.51306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}